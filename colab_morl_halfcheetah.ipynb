{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5fc2d",
   "metadata": {},
   "source": [
    "# Multi-Objective RL (HalfCheetah) - Google Colab Defteri\n",
    "\n",
    "Bu defter kurumsal düzeyde bir Çok Amaçlı Pekiştirmeli Öğrenme (Multi-Objective RL, MORL) pipeline'ını Google Colab üzerinde hızlıca çalıştırmak için hazırlanmıştır.\n",
    "\n",
    "İçerik:\n",
    "- HalfCheetah ortamı üzerine özel MORL wrapper (vektör ödül + normalizasyon + freeze)\n",
    "- PPO + GRPO (grup tabanlı grad projection) ajanı\n",
    "- Tercih (ağırlık) örnekleme (Dirichlet / köşeler)\n",
    "- Metrikler: Hypervolume (Monte Carlo & opsiyonel exact), IGD, IGD+\n",
    "- Embedding manifold (UMAP / t-SNE) üretimi (opsiyonel)\n",
    "- Toplu koşu ve özet tablo yapısına uyum\n",
    "- Colab'de MuJoCo kurulumu başarısız olursa düşen bir Dummy Env fallback\n",
    "\n",
    "Not: Colab'de GPU'yu (Runtime -> Change runtime type -> GPU) açmayı unutmayın.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dfab2",
   "metadata": {},
   "source": [
    "## 1. Gerekli Kütüphanelerin Kurulumu (requirements)\n",
    "Aşağıdaki hücre Colab ortamına gerekli paketleri kurar. MuJoCo artık gymnasium ile birlikte pip üzerinden kurulsa da Colab çekirdeklerindeki sürüm farklılıklarında hata alırsanız alternatif kurulumu deneyin.\n",
    "\n",
    "İlk deneme hızlı kurulum (minimum):\n",
    "```\n",
    "!pip install gymnasium mujoco pymoo numpy pandas matplotlib seaborn scikit-learn umap-learn python-ternary\n",
    "```\n",
    "\n",
    "Eğer depoda bir `requirements.txt` dosyanız varsa (Drive'a yüklemişseniz) şu şekilde de yapabilirsiniz:\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp /content/drive/MyDrive/patikaniz/requirements.txt .\n",
    "!pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Aşağıda otomatik tespit + kurulum yapan bir hücre vereceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b678d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otomatik kurulum\n",
    "%%bash\n",
    "pip install gymnasium mujoco pymoo numpy pandas matplotlib seaborn scikit-learn umap-learn python-ternary > /dev/null 2>&1\n",
    "\n",
    "echo \"Kurulum tamamlandi.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9f0d7",
   "metadata": {},
   "source": [
    "## 2. Kütüphanelerin İçe Aktarılması\n",
    "Kurulum tamamlandıysa gerekli modülleri içe aktaralım. MuJoCo kurulumu bazen GPU/renderer kurulumu gerektirebilir; hata alırsanız fallback env devreye girecek.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, random, time, csv, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# pymoo ve diğerleri\n",
    "try:\n",
    "    from pymoo.indicators.igd import IGD\n",
    "    from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
    "    try:\n",
    "        from pymoo.indicators.hv import HV as ExactHV\n",
    "    except Exception:\n",
    "        ExactHV = None\n",
    "except Exception as e:\n",
    "    print(\"pymoo import hatasi, bazı metrikler devre dışı:\", e)\n",
    "    IGD = None\n",
    "    NonDominatedSorting = None\n",
    "    ExactHV = None\n",
    "\n",
    "# MuJoCo / Gymnasium kurulumu test\n",
    "_use_dummy = False\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    _ = gym.make(\"HalfCheetah-v4\")\n",
    "    print(\"HalfCheetah-v4 basariyla yüklendi.\")\n",
    "except Exception as e:\n",
    "    print(\"Gerçek HalfCheetah yüklenemedi, DummyEnv kullanılacak:\", e)\n",
    "    _use_dummy = True\n",
    "    import gymnasium as gym\n",
    "\n",
    "class DummyHalfCheetah(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(17,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(-1.0, 1.0, shape=(6,), dtype=np.float32)\n",
    "        self.t = 0\n",
    "        self.max_t = 300\n",
    "        self.reward_space = gym.spaces.Box(-np.inf, np.inf, shape=(3,), dtype=np.float32)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.t = 0\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, {}\n",
    "    def step(self, action):\n",
    "        self.t += 1\n",
    "        # yapay hız, enerji ve smoothness\n",
    "        r_speed = np.random.randn()*0.1 + 1.0\n",
    "        r_energy = -0.1 * float(np.sum(np.square(action)))\n",
    "        r_smooth = -0.05 * float(np.sum(np.square(action)))\n",
    "        r_vec_raw = np.array([r_speed, r_energy, r_smooth], dtype=np.float32)\n",
    "        r_vec = (r_vec_raw - r_vec_raw.mean()) / (r_vec_raw.std()+1e-6)\n",
    "        done = self.t >= self.max_t\n",
    "        obs = np.random.randn(*self.observation_space.shape).astype(np.float32)\n",
    "        info = {\"reward_vec\": r_vec, \"reward_vec_raw\": r_vec_raw}\n",
    "        return obs, 0.0, done, False, info\n",
    "\n",
    "print(\"Dummy mod:\", _use_dummy)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84d4aa",
   "metadata": {},
   "source": [
    "## 3. Kodların Colab'a Uygun Hale Getirilmesi\n",
    "Bu bölümde depodaki Python dosyalarının minimal birleştirilmiş (inline) sürümünü kullanacağız. İsterseniz kendi Github/Drive kaynağınızdan `!wget` veya `!git clone` ile çekip import edebilirsiniz.\n",
    "\n",
    "Aşağıda çevrim içi (self-contained) bir MORL HalfCheetah wrapper (ve Dummy fallback), PPO+GRPO ajanı ve yardımcı fonksiyonlar veriliyor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135577f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MORL HalfCheetah Wrapper (sadeleştirilmiş) ===\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class RunningMeanStd:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, dtype=np.float64)\n",
    "        self.var = np.ones(shape, dtype=np.float64)\n",
    "        self.count = 1e-4\n",
    "    def update(self, x):\n",
    "        x = np.asarray(x, dtype=np.float64)\n",
    "        if x.ndim == 1: x = x[None, :]\n",
    "        batch_mean = x.mean(axis=0); batch_var = x.var(axis=0); batch_count = x.shape[0]\n",
    "        delta = batch_mean - self.mean; tot = self.count + batch_count\n",
    "        new_mean = self.mean + delta * (batch_count / tot)\n",
    "        m_a = self.var * self.count; m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + (delta**2) * self.count * batch_count / tot\n",
    "        self.mean, self.var, self.count = new_mean, M2 / tot, tot\n",
    "\n",
    "@dataclass\n",
    "class HCConfig:\n",
    "    speed_mode: str = \"target_speed\"\n",
    "    target_speed: float = 2.0\n",
    "    alpha_energy: float = 0.1\n",
    "    beta_smooth: float = 0.05\n",
    "    normalize: bool = True\n",
    "    norm_clip: float = 5.0\n",
    "    freeze_after_steps: int | None = 10000\n",
    "\n",
    "if not _use_dummy:\n",
    "    class HalfCheetahMORL(gym.Wrapper):\n",
    "        def __init__(self, cfg: HCConfig):\n",
    "            env = gym.make(\"HalfCheetah-v4\")\n",
    "            super().__init__(env)\n",
    "            self.cfg = cfg\n",
    "            self.dt = float(getattr(self.env.unwrapped, 'dt', 0.01))\n",
    "            self.prev_x = 0.0\n",
    "            self.prev_action = None\n",
    "            self.rms = RunningMeanStd((3,)) if cfg.normalize else None\n",
    "            self._norm_steps = 0\n",
    "            self.reward_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "        def reset(self, **kwargs):\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "            try: self.prev_x = float(self.env.unwrapped.data.qpos[0])\n",
    "            except Exception: self.prev_x = 0.0\n",
    "            self.prev_action = np.zeros(self.env.action_space.shape, dtype=np.float32)\n",
    "            return obs, info\n",
    "        def step(self, action):\n",
    "            obs, _, term, trunc, info = self.env.step(action)\n",
    "            try: x = float(self.env.unwrapped.data.qpos[0])\n",
    "            except Exception: x = 0.0\n",
    "            vx = (x - self.prev_x)/self.dt; self.prev_x = x\n",
    "            if self.cfg.speed_mode == 'target_speed':\n",
    "                r_speed = -abs(vx - self.cfg.target_speed)\n",
    "            else:\n",
    "                r_speed = vx\n",
    "            r_energy = -self.cfg.alpha_energy * np.sum(np.square(action))\n",
    "            r_smooth = -self.cfg.beta_smooth * np.sum(np.square(action - self.prev_action))\n",
    "            self.prev_action = action.astype(np.float32, copy=False)\n",
    "            r_vec_raw = np.array([r_speed, r_energy, r_smooth], dtype=np.float32)\n",
    "            r_vec = r_vec_raw.copy()\n",
    "            if self.rms is not None:\n",
    "                if not (self.cfg.freeze_after_steps and self._norm_steps >= self.cfg.freeze_after_steps):\n",
    "                    self.rms.update(r_vec)\n",
    "                self._norm_steps += 1\n",
    "                std = np.sqrt(np.clip(self.rms.var, 1e-6, None))\n",
    "                r_vec = (r_vec - self.rms.mean)/std\n",
    "                r_vec = np.clip(r_vec, -self.cfg.norm_clip, self.cfg.norm_clip)\n",
    "            info = dict(info); info['reward_vec'] = r_vec; info['reward_vec_raw'] = r_vec_raw\n",
    "            return obs, 0.0, term, trunc, info\n",
    "else:\n",
    "    # Dummy modunda reward_space zaten env icinde\n",
    "    HalfCheetahMORL = DummyHalfCheetah\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    if _use_dummy:\n",
    "        return HalfCheetahMORL()\n",
    "    return HalfCheetahMORL(HCConfig())\n",
    "\n",
    "# === Yardımcı Fonksiyonlar ===\n",
    "\n",
    "def update_global_nd(global_front: np.ndarray, new_points: np.ndarray):\n",
    "    if new_points.size == 0: return global_front\n",
    "    if global_front.size == 0: combined = new_points\n",
    "    else: combined = np.vstack([global_front, new_points])\n",
    "    if NonDominatedSorting is None:\n",
    "        # fallback: basit filtre (O(n^2))\n",
    "        nd = []\n",
    "        for i,p in enumerate(combined):\n",
    "            dominated = False\n",
    "            for j,q in enumerate(combined):\n",
    "                if j!=i and np.all(q>=p) and np.any(q>p):\n",
    "                    dominated = True; break\n",
    "            if not dominated: nd.append(p)\n",
    "        return np.array(nd, dtype=np.float32)\n",
    "    nds = NonDominatedSorting().do(combined, only_non_dominated_front=True)\n",
    "    return combined[nds]\n",
    "\n",
    "\n",
    "def compute_normalized_igd(global_front: np.ndarray, test_points: np.ndarray, min_ref: np.ndarray, max_ref: np.ndarray):\n",
    "    if global_front.size==0 or test_points.size==0 or IGD is None:\n",
    "        return np.nan, min_ref, max_ref\n",
    "    min_ref = np.minimum(min_ref, global_front.min(axis=0))\n",
    "    max_ref = np.maximum(max_ref, global_front.max(axis=0))\n",
    "    rng = max_ref - min_ref; rng[rng<=1e-9]=1.0\n",
    "    norm_ref = (global_front - min_ref)/rng\n",
    "    norm_test = (test_points - min_ref)/rng\n",
    "    igd_calc = IGD(norm_ref)\n",
    "    return float(igd_calc(norm_test)), min_ref, max_ref\n",
    "\n",
    "\n",
    "def monte_carlo_hv(norm_points: np.ndarray, samples: int = 3000):\n",
    "    if norm_points.size==0: return np.nan\n",
    "    d = norm_points.shape[1]\n",
    "    # ND filtre\n",
    "    nd = update_global_nd(np.empty((0,d),dtype=np.float32), norm_points).astype(np.float32)\n",
    "    U = np.random.rand(samples, d)\n",
    "    dominate = (nd[None,...] >= U[:,None,:]).all(axis=2).any(axis=1)\n",
    "    return float(dominate.mean())\n",
    "\n",
    "\n",
    "def exact_hv(points: np.ndarray, ref=None):\n",
    "    if ExactHV is None or points.size==0: return np.nan\n",
    "    try:\n",
    "        d = points.shape[1]\n",
    "        if ref is None: ref = np.zeros(d, dtype=np.float32)\n",
    "        hv = ExactHV(ref_point=ref)\n",
    "        return float(hv(points))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def igd_plus(ref: np.ndarray, approx: np.ndarray):\n",
    "    if ref.size==0 or approx.size==0: return np.nan\n",
    "    dists = []\n",
    "    for r in ref:\n",
    "        diff = approx - r\n",
    "        diff_pos = np.clip(diff, 0, None)\n",
    "        dists.append(np.linalg.norm(diff_pos, axis=1).min())\n",
    "    return float(np.mean(dists)) if dists else np.nan\n",
    "\n",
    "# === PPO + GRPO ===\n",
    "class FiLMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim):\n",
    "        super().__init__(); self.fc = nn.Sequential(nn.Linear(cond_dim,128), nn.ReLU(), nn.Linear(128,2*input_dim)); self.input_dim = input_dim\n",
    "    def forward(self, x, cond):\n",
    "        gb = self.fc(cond); g,b = gb[:,:self.input_dim], gb[:,self.input_dim:]\n",
    "        return g * x + b\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, w_dim, h=256):\n",
    "        super().__init__(); self.base = nn.Sequential(nn.Linear(s_dim,h), nn.Tanh()); self.film=FiLMLayer(h,w_dim); self.head=nn.Sequential(nn.Linear(h,h), nn.Tanh(), nn.Linear(h,a_dim)); self.action_var=nn.Parameter(torch.full((a_dim,),0.5))\n",
    "    def forward(self,s,w): h=self.base(s); h=self.film(h,w); return self.head(h)\n",
    "    def evaluate(self,s,a,w):\n",
    "        mean=self.forward(s,w); var=self.action_var.expand_as(mean); dist=MultivariateNormal(mean, torch.diag_embed(var)); lp=dist.log_prob(a); ent=dist.entropy(); return lp, mean, ent\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, w_dim, h=256):\n",
    "        super().__init__(); self.base=nn.Sequential(nn.Linear(s_dim,h), nn.Tanh()); self.film=FiLMLayer(h,w_dim); self.head=nn.Sequential(nn.Linear(h,h), nn.Tanh(), nn.Linear(h,1))\n",
    "    def forward(self,s,w): h=self.base(s); h=self.film(h,w); return self.head(h)\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions=[]; self.states=[]; self.prefs=[]; self.logprobs=[]; self.rewards=[]; self.raw_rewards=[]; self.is_terminals=[]\n",
    "    def clear(self):\n",
    "        self.actions.clear(); self.states.clear(); self.prefs.clear(); self.logprobs.clear(); self.rewards.clear(); self.raw_rewards.clear(); self.is_terminals.clear()\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, s_dim,a_dim,w_dim, hidden=256, lr=3e-4, betas=(0.9,0.999), gamma=0.99, K=10, eps_clip=0.2, ent_coef=0.01, use_grpo=True, group_mode='knn', knn_delta=0.15, gae_lambda=0.95, target_kl=0.02):\n",
    "        super().__init__(); self.gamma=gamma; self.eps_clip=eps_clip; self.K=K; self.ent_coef=ent_coef; self.use_grpo=use_grpo; self.group_mode=group_mode; self.knn_delta=knn_delta; self.gae_lambda=gae_lambda; self.target_kl=target_kl\n",
    "        self.actor=Actor(s_dim,a_dim,w_dim,hidden).to(device); self.old_actor=Actor(s_dim,a_dim,w_dim,hidden).to(device); self.old_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.critic=Critic(s_dim,w_dim,hidden).to(device); self.opt=torch.optim.Adam(list(self.actor.parameters())+list(self.critic.parameters()), lr=lr, betas=betas); self.mse=nn.MSELoss()\n",
    "    def select_action(self,s,w,memory:Memory):\n",
    "        s_t=torch.as_tensor(s,dtype=torch.float32,device=device).unsqueeze(0); w_t=torch.as_tensor(w,dtype=torch.float32,device=device).unsqueeze(0)\n",
    "        with torch.no_grad(): mean=self.old_actor(s_t,w_t); var=self.old_actor.action_var.expand_as(mean); dist=MultivariateNormal(mean, torch.diag_embed(var)); a=dist.sample(); lp=dist.log_prob(a)\n",
    "        memory.states.append(s_t); memory.prefs.append(w_t); memory.actions.append(a); memory.logprobs.append(lp); return a.squeeze(0).cpu().numpy()\n",
    "    def update(self,memory:Memory):\n",
    "        states=torch.cat(memory.states); actions=torch.cat(memory.actions); prefs=torch.cat(memory.prefs); old_logp=torch.cat(memory.logprobs).detach(); rewards=torch.as_tensor(memory.rewards,dtype=torch.float32,device=device); dones=torch.as_tensor(memory.is_terminals,dtype=torch.float32,device=device)\n",
    "        with torch.no_grad(): values=self.critic(states,prefs).squeeze(-1)\n",
    "        adv=torch.zeros_like(rewards); last=0.0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            next_non_term=1.0-(dones[t+1] if t<len(rewards)-1 else 0.0)\n",
    "            next_val=values[t+1] if t<len(rewards)-1 else 0.0\n",
    "            delta=rewards[t]+self.gamma*next_val*next_non_term - values[t]\n",
    "            last=delta + self.gamma*self.gae_lambda*next_non_term*last\n",
    "            adv[t]=last\n",
    "        returns=adv+values; adv=(adv-adv.mean())/(adv.std()+1e-8)\n",
    "        for _ in range(self.K):\n",
    "            logp,_,ent=self.actor.evaluate(states,actions,prefs); new_values=self.critic(states,prefs).squeeze(-1); ratios=torch.exp(logp-old_logp)\n",
    "            surr1=ratios*adv; surr2=torch.clamp(ratios,1-self.eps_clip,1+self.eps_clip)*adv; policy_core=-torch.min(surr1,surr2).mean(); value_loss=self.mse(new_values,returns); entropy_loss=ent.mean()\n",
    "            loss=policy_core+0.5*value_loss - self.ent_coef*entropy_loss\n",
    "            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "        self.old_actor.load_state_dict(self.actor.state_dict())\n",
    "        with torch.no_grad(): new_logp,_,_=self.actor.evaluate(states,actions,prefs)\n",
    "        kl=(old_logp-new_logp).mean().item(); return value_loss.item(), kl, float(1 - torch.var(returns-new_values)/(torch.var(returns)+1e-8))\n",
    "\n",
    "# === Tercih Örnekleme ===\n",
    "def sample_prefs(K=24,m=3,include_corners=True,seed=0):\n",
    "    rng=np.random.default_rng(seed); prefs=[]\n",
    "    if include_corners:\n",
    "        for i in range(m):\n",
    "            e=np.zeros(m,dtype=np.float32); e[i]=1.0; prefs.append(e)\n",
    "    while len(prefs)<K:\n",
    "        x=rng.random(m); x/=x.sum(); prefs.append(x.astype(np.float32))\n",
    "    return np.stack(prefs[:K],axis=0)\n",
    "\n",
    "print('Kod blokları yüklendi.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434f1fb",
   "metadata": {},
   "source": [
    "## 4. Örnek Eğitim & Metrik Hesaplama\n",
    "Bu bölümde birkaç yüz adımlık kısa bir eğitim döngüsü çalıştırıp metrikleri (IGD, IGD+, Monte Carlo HV) hesaplayacağız. Uzun koşular için `max_episodes` ve `update_timestep` değerlerini artırabilirsiniz. Colab zaman sınırı nedeniyle küçük bir demo yapılır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Küçük demo eğitimi\n",
    "max_episodes = 30\n",
    "update_timestep = 800  # adım bazlı güncelleme eşiği (yaklaşık 2-3 episode)\n",
    "max_ep_len = 400\n",
    "\n",
    "env = make_env()\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "if hasattr(env,'reward_space'):\n",
    "    pref_dim = env.reward_space.shape[0]\n",
    "else:\n",
    "    pref_dim = 3\n",
    "\n",
    "ppo = PPO(state_dim, action_dim, pref_dim, K=5)\n",
    "memory = Memory()\n",
    "\n",
    "train_prefs = sample_prefs(K=16, m=pref_dim, include_corners=True, seed=42)\n",
    "# Test tercihleri (farklı seed)\n",
    "test_prefs = sample_prefs(K=16, m=pref_dim, include_corners=True, seed=123)\n",
    "\n",
    "global_front = np.empty((0,pref_dim), dtype=np.float32)\n",
    "min_ref = np.full(pref_dim, np.inf, dtype=np.float32)\n",
    "max_ref = np.full(pref_dim, -np.inf, dtype=np.float32)\n",
    "\n",
    "results = []\n",
    "time_step = 0\n",
    "steps_since_update = 0\n",
    "\n",
    "for ep in range(1, max_episodes+1):\n",
    "    s, _ = env.reset()\n",
    "    # Eğitim tercihi rastgele seç\n",
    "    w = train_prefs[np.random.randint(len(train_prefs))]\n",
    "    ep_vec_norm = np.zeros(pref_dim, dtype=np.float32)\n",
    "    for t in range(max_ep_len):\n",
    "        a = ppo.select_action(s, w, memory)\n",
    "        s2, _, term, trunc, info = env.step(a)\n",
    "        done = term or trunc\n",
    "        r_vec = info.get('reward_vec') or info.get('reward_vec_raw')\n",
    "        scalar_r = float(np.dot(r_vec, w))\n",
    "        memory.rewards.append(scalar_r)\n",
    "        memory.raw_rewards.append(r_vec)\n",
    "        memory.is_terminals.append(done)\n",
    "        ep_vec_norm += r_vec\n",
    "        s = s2\n",
    "        time_step += 1\n",
    "        steps_since_update += 1\n",
    "        if steps_since_update >= update_timestep:\n",
    "            vloss, kl, ev = ppo.update(memory)\n",
    "            memory.clear()\n",
    "            steps_since_update = 0\n",
    "            # Test rollout metrikleri\n",
    "            test_returns = []\n",
    "            for tw in test_prefs:\n",
    "                st,_ = env.reset(); vec_sum = np.zeros(pref_dim, dtype=np.float32)\n",
    "                for _ in range(200):\n",
    "                    with torch.no_grad():\n",
    "                        mean = ppo.old_actor(torch.as_tensor(st,dtype=torch.float32,device=device).unsqueeze(0), torch.as_tensor(tw,dtype=torch.float32,device=device).unsqueeze(0))\n",
    "                        act = mean.squeeze(0).cpu().numpy()\n",
    "                    st2, _, d1, d2, inf2 = env.step(act)\n",
    "                    rv = inf2.get('reward_vec') or inf2.get('reward_vec_raw')\n",
    "                    vec_sum += rv\n",
    "                    st = st2\n",
    "                    if d1 or d2: break\n",
    "                test_returns.append(vec_sum)\n",
    "            test_returns = np.array(test_returns, dtype=np.float32)\n",
    "            igd_m, min_ref, max_ref = compute_normalized_igd(global_front, test_returns, min_ref, max_ref)\n",
    "            igd_p = igd_plus(global_front, test_returns) if global_front.size>0 else np.nan\n",
    "            if global_front.size>0:\n",
    "                rng = max_ref - min_ref; rng[rng<=1e-9]=1.0\n",
    "                train_norm = np.clip((global_front - min_ref)/rng,0,1)\n",
    "                hv_train = monte_carlo_hv(train_norm)\n",
    "            else: hv_train=np.nan\n",
    "            if test_returns.size>0 and global_front.size>0:\n",
    "                rng = max_ref - min_ref; rng[rng<=1e-9]=1.0\n",
    "                test_norm = np.clip((test_returns - min_ref)/rng,0,1)\n",
    "                hv_test = monte_carlo_hv(test_norm)\n",
    "            else: hv_test=np.nan\n",
    "            results.append(dict(episode=ep, vloss=vloss, kl=kl, ev=ev, igd=igd_m, igd_plus=igd_p, hv_train=hv_train, hv_test=hv_test))\n",
    "            print(f\"[Update] Ep {ep} KL {kl:.4f} IGD {igd_m:.4f} HV(train) {hv_train:.3f}\")\n",
    "        if done:\n",
    "            break\n",
    "    global_front = update_global_nd(global_front, ep_vec_norm.reshape(1,-1))\n",
    "\n",
    "print('Egitim tamamlandi. ND front boyutu:', len(global_front))\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa84860",
   "metadata": {},
   "source": [
    "### 4.1 Sonuçların Görselleştirilmesi\n",
    "Aşağıdaki hücre metriklerin zaman içindeki seyrini ve Pareto front yaklaşımını çizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f513648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "if not res_df.empty:\n",
    "    fig, axes = plt.subplots(1,3, figsize=(15,4))\n",
    "    res_df.plot(x='episode', y='igd', ax=axes[0], title='IGD')\n",
    "    res_df.plot(x='episode', y='hv_train', ax=axes[1], title='HV Train (MC)')\n",
    "    res_df.plot(x='episode', y='hv_test', ax=axes[2], title='HV Test (MC)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Sonuç DataFrame bos.')\n",
    "\n",
    "# ND front scatter (ilk iki amaç)\n",
    "if len(global_front)>0:\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(global_front[:,0], global_front[:,1], c=global_front[:,2], cmap='viridis')\n",
    "    plt.colorbar(label='Obj3')\n",
    "    plt.xlabel('Obj1'); plt.ylabel('Obj2'); plt.title('Yaklaşılan Pareto Noktaları')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d00f1",
   "metadata": {},
   "source": [
    "### 4.2 İleri Öneriler ve Hızlandırma\n",
    "- Daha uzun eğitim: `max_episodes` ve `update_timestep` artırın.\n",
    "- Çoklu seed: Aynı hücreyi farklı `random.seed()` ile döngüye alıp `pd.concat` ile özetleyin.\n",
    "- Profiling: `%%bash` ile `pip install torch-tb-profiler` + `torch.profiler` kullanılabilir.\n",
    "- Embedding manifold: Ek bir buffer ve UMAP / t-SNE hesaplaması ile aksiyon-preference uzayı incelenebilir.\n",
    "- Exact HV: Amaç sayısı <=3 ise `pymoo.indicators.hv.HV` kullanılabilir (hesaplama maliyetli olabilir).\n",
    "\n",
    "Defter burada temel uçtan uca akışı göstermektedir. Depodaki tam script (training_morl_example.py) daha kapsamlı logging + aggregate analiz içerir.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
